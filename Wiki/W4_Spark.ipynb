{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W4M1 - Building Apache Spark Standalone Cluster on Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "도커를 사용해 Apache Spark standalone cluster를 빌드하고, Spark job을 실행하고, 결과를 검증하기\n",
    "\n",
    "## 사전지식\n",
    "### 기능요구사항\n",
    "#### Cluster Setup:\n",
    "- 클러스터는 하나의 스파크 마스터와 두개의 스파크 worker 노드로 이루어져 있어야 한다.\n",
    "- 각 노드는 다른 노드들과 통신할 수 있어야 하며, 마스터는 worker에게 task를 스케줄링해주어야 한다.\n",
    "- job 진행과 클러스터 상태를 보기 위해 스파크 웹 UI 에 접근할 수 있어야 한다.\n",
    "\n",
    "#### Spark Job Execution:\n",
    "- Spark job은 mounted volume이나 특정 input 경로에서의 데이터셋을 읽을 수 있어야 한다. \n",
    "- job은 data transformation을 수행해야 하고(파이 예측 등) 특정 output 경로에 결과를 써야 한다.\n",
    "- output은 csv나 Parquet 등의 형태로 정확하게 나뉘고 저장되어야 한다.\n",
    "\n",
    "#### Error Handling:\n",
    "- setup은 도커 컨테이너와 설정 오류 등 네트워크 이슈들을 포함한 에러들을 적절하게 관리해야 한다.\n",
    "- 로그들은 spark job이나 cluster setup를 포함한 이슈들의 debug에 접근할 수 있어야 한다. \n",
    "\n",
    "#### Reproducibility:\n",
    "- 전체 setup은 제공된 Dockerfile과 docker-compose.yml을 사용해서 reproducible해야 한다. \n",
    "- Clear instruction이 제공되어야 한다.(Docker image build, start the cluster, submit the job, verify results 등)\n",
    "\n",
    "### 프로그래밍 요구사항\n",
    "#### Dockerfile:\n",
    "- 아파치 스파크로 도커 이미지를 빌드하는 도커파일을 생성해라\n",
    "- 도커 이미지는 Java, Python, Spark를 포함해야 한다\n",
    "- 이미지가 standalone mode로 설정되고 실행되는 것을 확인하여라\n",
    "\n",
    "#### Docker Compose File:\n",
    "- 하나의 마스터와 두 개의 worker node를 가지는 Spark Standalone cluster를 set up하는 docker-compose.yml 파일을 만들어라\n",
    "- 스파크 웹 UI와 master에 적절한 포트 개방을 위해 docker compose 파일을 설정해라\n",
    "\n",
    "#### Spark Job:\n",
    "- Spark distribution 파일에 포함된 sample job scripts 중 하나를 사용해라 (e.g., examples/src/main/python/pi.py)\n",
    "- 스크립트가 데이터셋을 읽고, transformation을 실행하는 지 확인하고(e.g., estimating π using the Monte Carlo method) 특정 output 위치에 결과를 쓰는지 확인해라\n",
    "\n",
    "#### Submission Script:\n",
    "- 셸 스크립트를 생성해서 Spark job을 제출해라 (spark-submit을 사용한 standalone cluster로) \n",
    "Create a shell script to submit the Spark job to the standalone cluster using spark-submit.\n",
    "\n",
    "### 예상결과 및 동작예시\n",
    "- Spark web UI로 job을 확인하여라 (http://localhost:8080)\n",
    "- output(파이의 예측값)은 Spark job의 로그로 print된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W4M2 - NYC TLC Trip Record Analysis using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "Apache Spark를 사용해서 'NYC Taxi and Limousine Commission (TLC) Trip Record Data.'를 활용하고 분석하기.\n",
    "데이터셋으로 여러 키 분석하는 Spark application을 파이썬으로 작성하기\n",
    "\n",
    "## 사전지식\n",
    "### 기능요구사항\n",
    "#### Data Ingestion:\n",
    "- 어플리케이션은 여러 달이나 년의 데이터를 효과적으로 수집해야 한다.\n",
    "- 다른 파일 포맷들(csv,parquet 등)을 다루어야 한다.\n",
    "\n",
    "#### Data Cleaning and Transformation:\n",
    "- missing values를 삭제하거나 대체한다.\n",
    "- 모든 관련 time field를 표준 timestamp 포맷으로 바꾸어라\n",
    "- 중요하지 않은(non-sensical) 값들은 걸러라(e.g., negative trip duration or distance)\n",
    "\n",
    "#### Metrics Calculation:\n",
    "- 평균 여행 기간\n",
    "- 평균 여행 거리\n",
    "- 결과가 저장되고 사람들이 읽기 쉽게 보여주도록 해라\n",
    "\n",
    "#### Peak Hours Analysis:\n",
    "- 시간당 여행 시작 수를 토대로 peak hours 를 정의해라 \n",
    "- 하루 내의 시간별 여행 분포를 시각화해라\n",
    "- 가장 많은 여행을 가지는 시간들을 강조해라\n",
    "\n",
    "#### Weather Condition Analysis:\n",
    "- 여행 수요와 날씨 조건(e.g., temperature, precipitation)을 연관시켜라\n",
    "- 다른 날씨 조건이 여행 수에 어떻게 영향을 미치는지 보여주어라\n",
    "- 발견을 증명하기 위해 statistical methods를 사용해라\n",
    "\n",
    "#### Output:\n",
    "- file (e.g., CSV, Parquet)을 저장하고 모든 계산된 식과 분석 결과를 포함해라\n",
    "- 시각화(e.g., bar charts, line graphs)를 생성해서 너의 발견을 뒷받침하여라\n",
    "\n",
    "### 프로그래밍 요구사항\n",
    "- Environment Setup:</br>\n",
    "Spark 환경이 정확히 설정되었는지 확인하기\n",
    "- Data Loading:</br>\n",
    "Spark DataFrame에 TLC Trip Record Data 로드하기\n",
    "- Data Cleaning:</br>\n",
    "결측치 등 데이터 정제하기\n",
    "- Calculation of Metrics:</br>\n",
    "average trip duration 계산하기</br>\n",
    "average trip distance 계산하기\n",
    "- Peak Hours Identification:</br>\n",
    "택시 사용량이 가장 많은 시간 알아내기\n",
    "- Weather Condition Analysis:</br>\n",
    "날씨 조건이 택시 수요에 영향을 미치는지 분석해라. </br>\n",
    "만약 시간에 따른 날씨 정보를 얻는 것이 필요하다면 추가적인 데이터셋을 사용해라 </br>\n",
    "발견을 시각화하기 위해 주피터노트북을 사용해라</br>\n",
    "\n",
    "### 팀 활동 요구사항\n",
    "- 만약 이 데이터가 사람이 운행하는 차량의 데이터가 아니라 '자율주행차' 데이터라면 어떤 Data Product를 만들면 좋을까? </br>\n",
    "- 아이디어를 수립하고 Prototype을 만들어 보아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pyspark.sql (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pyspark.sql\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
