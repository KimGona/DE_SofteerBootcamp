{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W5M1 - Data Analysis using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- TLC Trip Record Data 분석하며 스파크 역량 키우기. \n",
    "- 스파크가 내부적으로 어떻게 작동하는지 이해하기(RDD, DAG, lazy evaluation 등)\n",
    "\n",
    "## 사전지식\n",
    "The dataset can be found at NYC TLC Trip Record Data.\n",
    "### 기능요구사항\n",
    "#### Data Loading:\n",
    "- 특정 경로에서 데이터셋 읽어오기\n",
    "- 사용자가 원하는 csv, parquet등 다른 포맷 사용할 수 있게 하기\n",
    "\n",
    "#### Data Cleaning:\n",
    "- 결측값이나 이상값 기준 세우고 제거하기\n",
    "\n",
    "#### Transformation Logic:\n",
    "- fare amounts가 0이나 -인 값들 필터링하기\n",
    "- 관련 있는 컬럼들 데이터 map하고 적절한 데이터타입으로 바꾸기\n",
    "- total revenue와 trip 총 수 계산해서 데이터 reduce하기\n",
    "- 일별로 group by 해서 daily metrices 계산하기\n",
    "\n",
    "#### Aggregation Logic:\n",
    "- total number of trips 계산하고 보여주기\n",
    "- total revenue generated from the trips 계산하고 보여주기\n",
    "- 평균 trip distance 계산하고 보여주기\n",
    "- 하루당 trip 수 계산하고 보여주기\n",
    "- 하루당 총 revenue 계산하고 보여주기\n",
    "\n",
    "#### Performance Optimization:\n",
    "- 적절한 스파크 configuration사용해서 job performance 최적화하기\n",
    "- 스파크의 내장 함수와 job execution time 감소시키는 능력 사용하기\n",
    "\n",
    "#### Result Storage:\n",
    "- 최종 결과를 특정 위치에 저장해라\n",
    "- output이 사용자 친화적인 포맷으로 저장해라(csv, parquet등)\n",
    "\n",
    "## 프로그래밍 요구사항\n",
    "\n",
    "#### Spark Application Setup:\n",
    "- 파이썬으로 스파크 어플리케이션 작성해라\n",
    "- transformation과 action 하는 데 RDD api를 사용해라\n",
    "\n",
    "#### Data Ingestion:\n",
    "- 데이터셋을 RDD로 로드해라\n",
    "- 데이터를 분석하고 정제하기 위해 initial transformations를 실행해라\n",
    "\n",
    "#### Data Transformation:\n",
    "- 적어도 다섯개의 transformation을 수행해라(e.g., filtering, mapping, reducing, joining, aggregating)\n",
    "\n",
    "#### Data Aggregation:\n",
    "- 아래 값들 계산해라  \n",
    "Total number of trips.  \n",
    "Total revenue (sum of fare amounts).  \n",
    "Average trip distance.  \n",
    "Number of trips per day.  \n",
    "Total revenue per day.  \n",
    "\n",
    "#### Data Output:\n",
    "영구 저장소로 결과 저장해라(e.g., HDFS, S3, or local file system)\n",
    "\n",
    "### 예상결과 및 동작예시\n",
    "- Take a screenshot of the DAG visualization using Spark UI and include it in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W5M2 - Data Analysis using DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- TLC Trip Record Data 분석하며 스파크 역량 키우기. \n",
    "- 스파크가 내부적으로 어떻게 작동하는지 이해하기(DataFrame과 DAG)\n",
    "\n",
    "## 사전지식\n",
    "The dataset can be found at NYC TLC Trip Record Data. \n",
    "### 기능요구사항\n",
    "#### Data Loading:\n",
    "- TLC 데이터를 Spark DataFrame으로 로드하기\n",
    "- 데이터프레임의 스키마는 내재적으로 혹은 명시작으로 정의되어 있어야 한다.\n",
    "\n",
    "#### Data Cleaning:\n",
    "- 결측치나 이상치 기준 세우고 제거하기\n",
    "\n",
    "#### Data Transformations:\n",
    "- 다양한 transformations(filtering, aggregations, joins) 적용하여 데이터로부터 의미있는 인사이트 도출하기\n",
    "- transformation이 명료하고 최적화된 DAG를 만드는지 확인해라\n",
    "\n",
    "#### Data Actions:\n",
    "- actions 실행해서 transformation 실행하고 결과 얻어라\n",
    "- 결과를 저장해라\n",
    "\n",
    "#### Optimization:\n",
    "- caching과 persisting을 활용해서 stage 수를 최소화해서 execution plan을 최적화해라\n",
    "\n",
    "#### Documentation:\n",
    "- 레포트 작성해라(DAG 생성, transformations/actions 단계, 스파크 내부 매커니즘이 어떻게 작동하는지(lazy evaluation, stage optimization) 등)\n",
    "\n",
    "## 프로그래밍 요구사항\n",
    "### Read the Data:\n",
    "- TLC 데이터를 Spark DataFrame으로 로드하기\n",
    "- 데이터프레임의 스키마는 내재적으로 혹은 명시작으로 정의되어 있어야 한다.\n",
    "### Data Cleaning and Preprocessing:\n",
    "- null 값이나 이상치 가진 행 제거해라 (pickup and dropoff times, passenger count, trip distance 등 필수 컬럼에)\n",
    "- 비현실적인 값들 필터링해라(e.g., negative distances, excessively high trip distances)\n",
    "\n",
    "### Transformations:\n",
    "- DataFrame에서 최소 3개의 tranformation 실행해라(filter, aggregation, join 포함하여)  \n",
    "Filtering: Filter the trips based on certain criteria (e.g., trips with more than one passenger).  \n",
    "Aggregations: Calculate the total number of trips, average trip distance, and total revenue generated for a specific time period.  \n",
    "Joins: If using multiple datasets, join the trip data with another relevant dataset (e.g., weather data to analyze the impact of weather on trip durations).  \n",
    "### Actions:\n",
    "- 적어도 2개 이상의 action을 실행해라(collect, write 포함해서)  \n",
    "Collect: Collect a sample of the transformed data to the driver node.  \n",
    "Write: Save the final transformed DataFrame to a storage format of your choice (e.g., Parquet, CSV).  \n",
    "### Optimize DAG:\n",
    "- execution plan 최적화하고 stage 수 최소화하도록 transformation 재구성해라\n",
    "- 반복되는 연산이나 같은 DataFrame 최적화하기 위해 cache()나 persist() 사용해라\n",
    "\n",
    "### Lazy Evaluation:\n",
    "- action이 불리기 전에 transformation이 실행되지 않는 것을 보여주어서 lazy evaluation 개념을 증명해라\n",
    "\n",
    "## 예상결과 및 동작예시\n",
    "Take a screenshot of the DAG visualization using Spark UI and include it in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
